{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 6: Spring 2022\n",
    "\n",
    "Due: Friday, March 11th at midnight (12AM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary python libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For time checks\n",
    "import time\n",
    "\n",
    "# Scikit learn models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Cross-validation helpers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Polynomial generator\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Set random seed, so our results are similar\n",
    "# Where does 42 come from?\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate linear data experiments\n",
    "def genLinData(m,n,R2):\n",
    "    # y = x_1 + x_2 .. x_n + eps\n",
    "    # X's scaled so the variance of explained part is 1\n",
    "    # Function choses variance of noise to set theoretical R2 = R2 argument\n",
    "    sigNoise = np.sqrt(1./n)\n",
    "    X = np.random.normal(size=(m,n),loc=0,scale=sigNoise)\n",
    "    noise = np.sqrt((1.-R2)/R2)\n",
    "    eps = np.random.normal(size=m,loc=0,scale=noise)\n",
    "    y = np.sum(X,axis=1)+eps\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 65)\n"
     ]
    }
   ],
   "source": [
    "# Set up initial monte-carlo simulation\n",
    "X, y = genLinData(60,10,0.75)\n",
    "\n",
    "# Expand features(X) using 2-degree polynomials\n",
    "poly = PolynomialFeatures(degree=2,include_bias=False)\n",
    "Xpoly = poly.fit_transform(X)\n",
    "# A reminder of how big Xpoly is (remember it has cross terms like x1*x2, not just x1^2)\n",
    "# n + n + n(n-1)/2 = 65\n",
    "print(Xpoly.shape)\n",
    "\n",
    "# Set up basic simulation parameters\n",
    "nmc = 50 # number of randomized cross-validations\n",
    "testSize = 0.25 # Test set fraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear regression\n",
    "\n",
    "Run a monte-carlo cross-validation for a basic linear regression model using the nmc, and testSize parameters, and cross_Validate() as we usually do.  Report the mean score from both the training and test samples.  Do this for both the (y,X) pair, and the (y,Xpoly) pair.  In each case also report the elapsed time for your entire randomized cross validation experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.07481551170349121\n",
      "0.7843937702979177\n",
      "0.555999850455543\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import time\n",
    "\n",
    "lr = LinearRegression()\n",
    "nmc = 50 # number of randomized cross-validations\n",
    "testSize = 0.25 # Test set fraction\n",
    "stime = time.time()\n",
    "shuffle = ShuffleSplit(n_splits=nmc, test_size=.25)\n",
    "CVInfo = cross_validate(lr, X, y, cv=shuffle,return_train_score=True)\n",
    "\n",
    "\n",
    "etime = time.time()\n",
    "print(\"Elapsed time:\",etime-stime)\n",
    "print(np.mean(CVInfo['train_score']))\n",
    "print(np.mean(CVInfo['test_score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.10967302322387695\n",
      "1.0\n",
      "-1.1696686727079508\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "nmc = 50 # number of randomized cross-validations\n",
    "testSize = 0.25 # Test set fraction\n",
    "\n",
    "stime = time.time()\n",
    "shuffle = ShuffleSplit(n_splits=nmc, test_size=.25)\n",
    "CVInfo = cross_validate(lr, Xpoly, y, cv=shuffle,return_train_score=True)\n",
    "\n",
    "etime = time.time()\n",
    "print(\"Elapsed time:\",etime-stime)\n",
    "print(np.mean(CVInfo['train_score']))\n",
    "print(np.mean(CVInfo['test_score']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stochastic gradient\n",
    "\n",
    "Now estimate the model using a stochastic gradient system.  Use the scikit-Learn function SGDRegressor().\n",
    "Set it up with the following parameters.\n",
    "\n",
    "SGDRegressor(max_iter=500000, tol=1e-6,penalty=\"None\",eta0=0.01,\n",
    "                    alpha=0.0,learning_rate=\"adaptive\")\n",
    "\n",
    "Run a randomized cross-validation for both the (y,X) and (y,Xpoly) pairs using the nmc, testSize that you used in the last simulation.  Report the mean train and test scores in both cases.\n",
    "\n",
    "Also, find the elapsed time for your entire randomized cross validation run.\n",
    "\n",
    "Finally, try boosting the learning rate, eta, to 1.0 for the (y,Xpoly) case. How did that work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.11567330360412598\n",
      "0.7817294515574925\n",
      "0.5459274008951447\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation helpers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "nmc = 50 # number of randomized cross-validations\n",
    "testSize = 0.25 # Test set fraction\n",
    "stime = time.time()\n",
    "sgd_reg = SGDRegressor(max_iter=500000, tol=1e-6, penalty=None, eta0=0.01,alpha=0.0,learning_rate=\"adaptive\")\n",
    "\n",
    "sgd_reg.fit(X, y)\n",
    "shuffle = ShuffleSplit(n_splits=nmc, test_size=testSize)\n",
    "CVInfo = cross_validate(sgd_reg, X, y, cv=shuffle,return_train_score=True)\n",
    "etime = time.time()\n",
    "print(\"Elapsed time:\",etime-stime)\n",
    "print(np.mean(CVInfo['train_score']))\n",
    "print(np.mean(CVInfo['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2.486431121826172\n",
      "0.9884705550319387\n",
      "-0.05924810560224368\n"
     ]
    }
   ],
   "source": [
    "nmc = 50 # number of randomized cross-validations\n",
    "testSize = 0.25 # Test set fraction\n",
    "sgd_reg = SGDRegressor(max_iter=500000, tol=1e-6, penalty=None, eta0=0.01,alpha=0.0,learning_rate=\"adaptive\")\n",
    "stime = time.time()\n",
    "sgd_reg.fit(Xpoly, y)\n",
    "shuffle = ShuffleSplit(n_splits=nmc, test_size=testSize)\n",
    "CVInfo = cross_validate(sgd_reg, Xpoly, y, cv=shuffle,return_train_score=True)\n",
    "etime = time.time()\n",
    "print(\"Elapsed time:\",etime-stime)\n",
    "print(np.mean(CVInfo['train_score']))\n",
    "print(np.mean(CVInfo['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.09374785423278809\n",
      "-584389931.804977\n",
      "-801913092.1663612\n"
     ]
    }
   ],
   "source": [
    "#Change learning rate to 1\n",
    "nmc = 50 # number of randomized cross-validations\n",
    "testSize = 0.25 # Test set fraction\n",
    "sgd_reg = SGDRegressor(max_iter=500000, tol=1e-6, penalty=None, eta0=1.0,alpha=0.0,learning_rate=\"adaptive\")\n",
    "stime = time.time()\n",
    "sgd_reg.fit(Xpoly, y)\n",
    "shuffle = ShuffleSplit(n_splits=nmc, test_size=testSize)\n",
    "CVInfo = cross_validate(sgd_reg, Xpoly, y, cv=shuffle,return_train_score=True)\n",
    "etime = time.time()\n",
    "print(\"Elapsed time:\",etime-stime)\n",
    "print(np.mean(CVInfo['train_score']))\n",
    "print(np.mean(CVInfo['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elapsed time decreases after the learning rate is increased from 0.01 to 1.0 but the both the train and test score is negative and indicates that we set a the learning rate too large and there is unstable convergence due to large learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Early stopping\n",
    "\n",
    "Now, implement an early stopping procedure for the stochastic gradient estimation.  Use the previous parameters for SGDRegressor(), but turn on early_stopping.  Set the validation_fraction to 1/3 of the data set.  Again, run the same randomized cross valiation experiment that you have done before, and report mean scores on training and test data.  Repeat this for (y,X) and (y,Xpoly).  Also, report the elapsed time in for each set of nmc simulations.\n",
    "\n",
    "Note: Both of these can be pretty slow.  On my laptop machine (5 years old) the (y,Xpoly) run is taking about 5-10 minutes.  Run experiments on your code with much lower values for nmc.  Then when you know things are working take it back to the original value.  It is interesting that early stopping is so slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 4.616688966751099\n",
      "0.6314160914116245\n",
      "0.39122643273772\n"
     ]
    }
   ],
   "source": [
    "nmc = 50 # number of randomized cross-validations\n",
    "testSize = 0.25 # Test set fraction\n",
    "sgd_reg = SGDRegressor(max_iter=500000, tol=1e-6, penalty=None, eta0=0.01,alpha=0.0,\n",
    "                       early_stopping=True,validation_fraction=1./3,learning_rate=\"adaptive\")\n",
    "stime = time.time()\n",
    "sgd_reg.fit(X, y)\n",
    "shuffle = ShuffleSplit(n_splits=nmc, test_size=testSize)\n",
    "CVInfo = cross_validate(sgd_reg, X, y, cv=shuffle,return_train_score=True)\n",
    "etime = time.time()\n",
    "print(\"Elapsed time:\",etime-stime)\n",
    "print(np.mean(CVInfo['train_score']))\n",
    "print(np.mean(CVInfo['test_score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 110.02876305580139\n",
      "0.6384744377101196\n",
      "0.2460967790231116\n"
     ]
    }
   ],
   "source": [
    "# Set up randomized cross validation (y,Xpoly) pair\n",
    "nmc = 50 # number of randomized cross-validations\n",
    "testSize = 0.25 # Test set fraction\n",
    "sgd_reg = SGDRegressor(max_iter=500000, tol=1e-6, penalty=None, eta0=0.01,alpha=0.0,\n",
    "                       early_stopping=True,validation_fraction=1./3,learning_rate=\"adaptive\")\n",
    "stime = time.time()\n",
    "sgd_reg.fit(Xpoly, y)\n",
    "shuffle = ShuffleSplit(n_splits=nmc, test_size=testSize)\n",
    "CVInfo = cross_validate(sgd_reg, Xpoly, y, cv=shuffle,return_train_score=True)\n",
    "etime = time.time()\n",
    "print(\"Elapsed time:\",etime-stime)\n",
    "print(np.mean(CVInfo['train_score']))\n",
    "print(np.mean(CVInfo['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Batch gradient descent\n",
    "\n",
    "Estimate the model using a batch gradient descent model.  See the \"earlyStopping\" notebook for examples on how to implement this.  (It is tricky in terms of code.)\n",
    "\n",
    "Do a randomized cross-validation here using the same parameters as before, but this will be tricky because of the way the model gets implemented.  You cannot use cross_validate().  You need to go back to train_test_split(), and loop over the train/test draws as we did the first time in the class with this.  Also, for the batch estimation, use 1000 epochs (or passes through the data).\n",
    "\n",
    "Again, report mean scores on training and testing data, and elapsed time for the entire procedure.\n",
    "\n",
    "In this case, only use the (y,Xpoly) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 7.8689868450164795\n",
      "0.9941305123388869\n",
      "0.8913391935299978\n"
     ]
    }
   ],
   "source": [
    "sgd_reg = SGDRegressor(max_iter=500000, tol=1e-6, penalty=None, eta0=0.01,alpha=0.0,learning_rate=\"adaptive\")\n",
    "nmc = 50 # number of randomized cross-validations\n",
    "testSize = 0.25 # Test set fraction\n",
    "stime = time.time()\n",
    "testScore = np.zeros(nmc)\n",
    "trainScore = np.zeros(nmc)\n",
    "for i in range(nmc):\n",
    "    Xpoly_train, Xpoly_test, y_train, y_test = train_test_split(Xpoly,y,test_size=0.25)\n",
    "    for epoch in range(1000):\n",
    "        sgd_reg.partial_fit(Xpoly_train,y_train)\n",
    "    trainScore[i] = sgd_reg.score(Xpoly_train,y_train)\n",
    "    testScore[i]  = sgd_reg.score(Xpoly_test,y_test)\n",
    "\n",
    "etime = time.time()\n",
    "print(\"Elapsed time:\",etime-stime)\n",
    "print(np.mean(trainScore))\n",
    "print(np.mean(testScore))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ridge regression\n",
    "\n",
    "Now try a ridge regresion with (y, Xpoly) data only.  Perform a grid search (using a for loop) with alpha in the following range.\n",
    "\n",
    "np.arange(start=0.1,stop=3.0,step=0.1)\n",
    "\n",
    "For each alpha find the mean train and test scores (R-squared) from a randomized cross-validation (same params as before).\n",
    "\n",
    "Plot the test and train mean scores against the alpha values.\n",
    "\n",
    "Record your bestAlpha from the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "testScoreList = []\n",
    "trainScoreList = []\n",
    "nmc = 50\n",
    "shuffle = ShuffleSplit(n_splits=nmc, test_size=.25)\n",
    "# List of C's to try\n",
    "alphas = np.arange(start=0.1,stop=3.0,step=0.1)\n",
    "for a in alphas:\n",
    "    fullModel = make_pipeline(StandardScaler(), Ridge(alpha=a))\n",
    "    CVInfo = cross_validate(fullModel, Xpoly, y, cv=shuffle,return_train_score=True,n_jobs=-1)\n",
    "    trainScoreList.append(np.mean(CVInfo['train_score']))\n",
    "    testScoreList.append(np.mean(CVInfo['test_score']))\n",
    "\n",
    "# plotting\n",
    "plt.plot(alphas,trainScoreList,label='Mean Train Score')\n",
    "plt.plot(alphas,testScoreList,label='Mean Test Score')\n",
    "plt.xlabel('Alpha')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Mean Test Score -0.19772168312331323\n",
      "Best Alpha: 2.6\n"
     ]
    }
   ],
   "source": [
    "best_index = testScoreList.index(max(testScoreList))\n",
    "print(\"Best Mean Test Score\", max(testScoreList))\n",
    "bestAlpha = round(alphas[best_index],1) \n",
    "print(\"Best Alpha:\", bestAlpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ridge regression summary\n",
    "\n",
    "Run our usual randomized monte-carlo trial for ridge regression using your bestAlpha value.  Same parameters as before.  Do this with only the (y,Xpoly) pairs and report the mean train and test scores.  Also, report the elapsed time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(y, Xpoly) pair\n",
      "Mean Train Score: 0.7155116537732595\n",
      "Mean Test Score: 0.41615276416436087\n",
      "Elapsed Time: 0.10569024085998535\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "ridge = Ridge(alpha=bestAlpha)\n",
    "CVInfo = cross_validate(ridge, Xpoly, y, cv=shuffle,return_train_score=True)\n",
    "etime = time.time()\n",
    "rtimeP = etime-stime\n",
    "print(\"\\n(y, Xpoly) pair\")\n",
    "print(\"Mean Train Score:\", np.mean(CVInfo['train_score']))\n",
    "print(\"Mean Test Score:\", np.mean(CVInfo['test_score']))\n",
    "print(\"Elapsed Time:\",rtimeP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Discsussion\n",
    "\n",
    "Compare your results from your various models for the (y,Xpoly) case.  Which model performs best in terms of test R-squared?  What is your overall ranking of models?  (LinearRegression, Stochastic Gradient Descent, Stochastic Gradient Descent with Early Stopping, Batch Gradient Descent, and Ridge regression)\n",
    "\n",
    "In terms of compute time how do the methods compare?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression\n",
    "Elapsed time: 0.10967302322387695\n",
    "Test Score: -1.1696686727079508\n",
    "\n",
    "Stochastic Gradient Descent\n",
    "Elapsed time: 2.486431121826172\n",
    "Test Score: -0.05924810560224368\n",
    "\n",
    "Stochastic Gradient Descent with Early Stopping\n",
    "Elapsed time: 110.02876305580139\n",
    "Test Score: 0.2460967790231116\n",
    "\n",
    "Batch Gradient Descent\n",
    "Elapsed time: 7.8689868450164795\n",
    "Test Score: 0.8913391935299978\n",
    "\n",
    "Ridge Regression\n",
    "Mean Test Score: 0.41615276416436087\n",
    "Elapsed Time: 0.10569024085998535\n",
    "\n",
    "In terms of test R-squared, Batch Gradient Descent performs best.\n",
    "The overall rankings of the model would be:\n",
    "\n",
    "1) Batch Gradient Descent\n",
    "2) Ridge Regression 3) Stochastic Gradient Descent with Early Stopping 4) Stochastic Gradient Descent 5) Linear Regression\n",
    "\n",
    "In terms of compute time, the rankings would be:\n",
    "\n",
    "1) Ridge Regression 2) Linear Regression 3) Stochastic Gradient Descent 4) Batch Gradient Descent 5) Stochastic Gradient Descent with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
